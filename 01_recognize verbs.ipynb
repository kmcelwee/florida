{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import en_core_web_sm\n",
    "import nltk\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/top_data.json') as f:\n",
    "    j = json.load(f)\n",
    "\n",
    "ids_dropped = set(\n",
    "    [\n",
    "        '3nkl4l', 'etfdib', '312wku', 'bc0gjg', '95gk6u', '34ey1p', '3wjg2o', '51y9gl', '83n2z2',\n",
    "        'ex5nam', '3xbynm', '4030de', '514s33', '3mx6p8', '2u6yyd', '9oeth4', '9ccilf', '3x2lnb', \n",
    "        '52yazt', '558t8b', '1pu47m', 'el5ny5', '56woyb'\n",
    "    ]\n",
    ")\n",
    "\n",
    "new_j = []\n",
    "for i, d in enumerate(j):\n",
    "    d['title'] = (d['title']\n",
    "        .lower()\n",
    "        .replace(' | abc7.com', '')\n",
    "        .replace(' – wsvn 7news | miami news, weather, sports', '')\n",
    "        .replace('floridaman', 'florida man')\n",
    "        .replace('floridawoman', 'florida woman')\n",
    "        .replace('\\n', ' ')\n",
    "        .replace(\"[x-post /r/funny]\", '')\n",
    "        .replace('my result for the florida man challenge: ', '')\n",
    "        .replace(\"floridamen\", 'florida men')\n",
    "        .replace('x-post from r/nottheonion :', '')\n",
    "        .replace('(x-post r/funny)', '')\n",
    "        .replace('(x-post from /r/thesquadonpoint)', '')\n",
    "        .replace(' [this should be our new sidebar picture]', '')\n",
    "        .strip()\n",
    "    )\n",
    "    \n",
    "    d['time'] = datetime.datetime.fromtimestamp(d['time']).date().strftime('%B %d, %Y')\n",
    "    \n",
    "    \n",
    "    if (not (d['id'] in ids_dropped)) and (not (d['domain'] == 'self.FloridaMan')):\n",
    "        new_j.append(d)\n",
    "j = new_j\n",
    "\n",
    "titles = [d['title'] for d in j]\n",
    "\n",
    "with open('data/titles.txt', 'w') as f:\n",
    "    f.write('\\n'.join(sorted(titles)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parse all verbs\n",
    "nlp = en_core_web_sm.load()\n",
    "def get_verb(s):\n",
    "    m = [x.root.head.text for x in nlp(s).noun_chunks if x.root.head.pos_ == 'VERB']\n",
    "    standardized = [WordNetLemmatizer().lemmatize(x, 'v') for x in m]\n",
    "    remove = set(['d', \"’re\", \"’m\", \"’s\"])\n",
    "    filtered = [x for x in standardized if x not in remove]\n",
    "    return None if len(filtered) == 0 else list(set(filtered))\n",
    "\n",
    "for d in j:\n",
    "    d.update({'verbs': get_verb(d['title'])})\n",
    "verb_json = j\n",
    "del j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make manual mapper\n",
    "with open('mappers/verb_mapper_manual.txt') as f:\n",
    "    m = [x.strip() for x in f.readlines()]\n",
    "\n",
    "d = {}\n",
    "for x in m:\n",
    "    verb, title = x.split(' => ')\n",
    "    d[title] = [verb]\n",
    "\n",
    "with open('mappers/verb_mapper_manual.json', 'w') as f:\n",
    "    json.dump(d, f, indent=4)\n",
    "\n",
    "manual_mapper = d\n",
    "\n",
    "def apply_mapper(x):\n",
    "    return manual_mapper.get(x['title'], x['verbs'])\n",
    "\n",
    "for x in verb_json:\n",
    "    x['verbs'] = apply_mapper(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# separate into man/woman\n",
    "fm = [x for x in verb_json if 'florida man' in x['title'] or 'florida men' in x['title']]\n",
    "fw = [x for x in verb_json if 'florida woman' in x['title'] or 'florida women' in x['title']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build final jsons\n",
    "for j, name in zip([fm, fw], ['fm', 'fw']):\n",
    "    d = {}\n",
    "    d['count'] = len(j)\n",
    "    d['verbs'] = {}\n",
    "    \n",
    "    # expand to one set\n",
    "    l = []\n",
    "    for x in j:\n",
    "        l.extend(x['verbs'])\n",
    "    verbs = set(l)\n",
    "    \n",
    "    for verb in verbs:\n",
    "        content = sorted([x for x in j if verb in x['verbs']], key=lambda x: -x['score'])\n",
    "        d['verbs'][verb] = {}\n",
    "        d['verbs'][verb]['articles'] = content\n",
    "        d['verbs'][verb]['count'] = len(content)\n",
    "\n",
    "    with open(f'{name}.json', 'w') as f:\n",
    "        json.dump(d, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
